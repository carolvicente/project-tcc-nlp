{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Text Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['datasets/el_pais/all_news.txt', 'datasets/el_pais/all_news2.txt', \n",
    "                  'datasets/el_pais/all_news3.txt', 'datasets/el_pais/all_news4.txt',\n",
    "                 'datasets/folha/all_news.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'datasets/el_pais/all_news.txt'...\n",
      "Corpus is now 2217835 characters long\n",
      "Reading 'datasets/el_pais/all_news2.txt'...\n",
      "Corpus is now 4407384 characters long\n",
      "Reading 'datasets/el_pais/all_news3.txt'...\n",
      "Corpus is now 5466038 characters long\n",
      "Reading 'datasets/el_pais/all_news4.txt'...\n",
      "Corpus is now 6173125 characters long\n",
      "Reading 'datasets/folha/all_news.txt'...\n",
      "Corpus is now 6554289 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for filenames in filenames:\n",
    "    print(\"Reading '{0}'...\".format(filenames))\n",
    "    with codecs.open(filenames, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Lower Case Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49801 Frases\n",
      "a nova pesquisa datafolha, divulgada na noite desta sexta-feira, confirma as tendências apontadas por outras pesquisas: fernando haddad (pt) segue em sua ascensão, desta vez com 6 pontos além do último levantamento.\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_sentences),'Frases')\n",
    "print(raw_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Remove Ponctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Zçáíúéóàêôãõ]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 1,065,301 words\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(s) for s in sentences])\n",
    "print(\"The book corpus contains {0:,} words\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a nova pesquisa datafolha, divulgada na noite desta sexta-feira, confirma as tendências apontadas por outras pesquisas: fernando haddad (pt) segue em sua ascensão, desta vez com 6 pontos além do último levantamento.\n",
      "['a', 'nova', 'pesquisa', 'datafolha', 'divulgada', 'na', 'noite', 'desta', 'sexta', 'feira', 'confirma', 'as', 'tendências', 'apontadas', 'por', 'outras', 'pesquisas', 'fernando', 'haddad', 'pt', 'segue', 'em', 'sua', 'ascensão', 'desta', 'vez', 'com', 'pontos', 'além', 'do', 'último', 'levantamento']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[0])\n",
    "print(sentence_to_wordlist(raw_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"datasets/stopwords.txt\", \"r\")\n",
    "lines = text_file.readlines()\n",
    "lines = list(map(lambda x: x.replace('\\n','').replace(' ',''), lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    new_sentences.append(list(filter(lambda x: x not in lines , s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nova', 'pesquisa', 'datafolha', 'divulgada', 'noite', 'desta', 'sexta', 'feira', 'confirma', 'tendências', 'apontadas', 'outras', 'pesquisas', 'fernando', 'haddad', 'pt', 'segue', 'ascensão', 'desta', 'vez', 'pontos', 'além', 'último', 'levantamento']\n"
     ]
    }
   ],
   "source": [
    "print(new_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 596,840 words\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in new_sentences])\n",
    "print(\"The book corpus contains {0:,} words\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "num_features  \n",
    "min_word_count  \n",
    "num_workers  \n",
    "context_size  \n",
    "downsampling  \n",
    "seed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Dimensionality of Word Vectors\n",
    "# More dimensions, more computationally expensive to train but also more accurate\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "# More workers == More faster\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "print(num_workers)\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "# random number generator\n",
    "# deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sg=1\n",
    "skip-gram is used; \n",
    "\n",
    "sg=0\n",
    "CBOW is used;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow2vec = w2v.Word2Vec(\n",
    "    sg=0,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 15:02:38,694 : INFO : collecting all words and their counts\n",
      "2019-01-12 15:02:38,695 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-12 15:02:38,748 : INFO : PROGRESS: at sentence #10000, processed 116703 words, keeping 16607 word types\n",
      "2019-01-12 15:02:38,813 : INFO : PROGRESS: at sentence #20000, processed 235511 words, keeping 24550 word types\n",
      "2019-01-12 15:02:38,892 : INFO : PROGRESS: at sentence #30000, processed 351021 words, keeping 29974 word types\n",
      "2019-01-12 15:02:38,960 : INFO : PROGRESS: at sentence #40000, processed 469446 words, keeping 34182 word types\n",
      "2019-01-12 15:02:39,013 : INFO : collected 38727 word types from a corpus of 596840 raw words and 49801 sentences\n",
      "2019-01-12 15:02:39,014 : INFO : Loading a fresh vocabulary\n",
      "2019-01-12 15:02:39,189 : INFO : effective_min_count=3 retains 17786 unique words (45% of original 38727, drops 20941)\n",
      "2019-01-12 15:02:39,190 : INFO : effective_min_count=3 leaves 570101 word corpus (95% of original 596840, drops 26739)\n",
      "2019-01-12 15:02:39,279 : INFO : deleting the raw counts dictionary of 38727 items\n",
      "2019-01-12 15:02:39,281 : INFO : sample=0.001 downsamples 15 most-common words\n",
      "2019-01-12 15:02:39,282 : INFO : downsampling leaves estimated 565230 word corpus (99.1% of prior 570101)\n",
      "2019-01-12 15:02:39,368 : INFO : estimated required memory for 17786 words and 300 dimensions: 51579400 bytes\n",
      "2019-01-12 15:02:39,369 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model_cbow2vec.build_vocab(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 17786\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(model_cbow2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49801"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow2vec.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 15:03:44,400 : INFO : training model with 4 workers on 17786 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
      "2019-01-12 15:03:45,430 : INFO : EPOCH 1 - PROGRESS: at 88.29% examples, 486147 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 15:03:45,524 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 15:03:45,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 15:03:45,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 15:03:45,583 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 15:03:45,584 : INFO : EPOCH - 1 : training on 596840 raw words (565258 effective words) took 1.2s, 484047 effective words/s\n",
      "2019-01-12 15:03:46,615 : INFO : EPOCH 2 - PROGRESS: at 64.60% examples, 358687 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 15:03:47,064 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 15:03:47,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 15:03:47,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 15:03:47,117 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 15:03:47,118 : INFO : EPOCH - 2 : training on 596840 raw words (565159 effective words) took 1.5s, 374927 effective words/s\n",
      "2019-01-12 15:03:48,141 : INFO : EPOCH 3 - PROGRESS: at 95.48% examples, 526258 words/s, in_qsize 4, out_qsize 0\n",
      "2019-01-12 15:03:48,201 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 15:03:48,204 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 15:03:48,223 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 15:03:48,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 15:03:48,247 : INFO : EPOCH - 3 : training on 596840 raw words (565162 effective words) took 1.1s, 507665 effective words/s\n",
      "2019-01-12 15:03:49,279 : INFO : EPOCH 4 - PROGRESS: at 86.73% examples, 477633 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 15:03:49,417 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 15:03:49,432 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 15:03:49,469 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 15:03:49,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 15:03:49,482 : INFO : EPOCH - 4 : training on 596840 raw words (565267 effective words) took 1.2s, 465478 effective words/s\n",
      "2019-01-12 15:03:50,520 : INFO : EPOCH 5 - PROGRESS: at 66.57% examples, 366466 words/s, in_qsize 7, out_qsize 1\n",
      "2019-01-12 15:03:50,842 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 15:03:50,881 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 15:03:50,883 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 15:03:50,902 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 15:03:50,903 : INFO : EPOCH - 5 : training on 596840 raw words (565240 effective words) took 1.4s, 406174 effective words/s\n",
      "2019-01-12 15:03:50,904 : INFO : training on a 2984200 raw words (2826086 effective words) took 6.5s, 434654 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2826086, 2984200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow2vec.train(new_sentences,epochs=5, total_examples=model_cbow2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 15:04:33,332 : INFO : saving Word2Vec object under trained/model_cbow2vec.w2v, separately None\n",
      "2019-01-12 15:04:33,335 : INFO : not storing attribute vectors_norm\n",
      "2019-01-12 15:04:33,337 : INFO : not storing attribute cum_table\n",
      "2019-01-12 15:04:34,125 : INFO : saved trained/model_cbow2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "model_cbow2vec.save(os.path.join(\"trained\", \"model_cbow2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 15:04:57,675 : INFO : loading Word2Vec object from trained/model_cbow2vec.w2v\n",
      "2019-01-12 15:04:58,061 : INFO : loading wv recursively from trained/model_cbow2vec.w2v.wv.* with mmap=None\n",
      "2019-01-12 15:04:58,062 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-01-12 15:04:58,064 : INFO : loading vocabulary recursively from trained/model_cbow2vec.w2v.vocabulary.* with mmap=None\n",
      "2019-01-12 15:04:58,065 : INFO : loading trainables recursively from trained/model_cbow2vec.w2v.trainables.* with mmap=None\n",
      "2019-01-12 15:04:58,070 : INFO : setting ignored attribute cum_table to None\n",
      "2019-01-12 15:04:58,073 : INFO : loaded trained/model_cbow2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "model2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"model_cbow2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-01-12 15:10:58,338 : INFO : precomputing L2-norms of word weight vectors\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('inácio', 0.9295151233673096),\n",
       " ('petista', 0.9198875427246094),\n",
       " ('impugnado', 0.915873646736145),\n",
       " ('inelegível', 0.8941935300827026),\n",
       " ('candidatura', 0.8923618197441101),\n",
       " ('preso', 0.8914204835891724),\n",
       " ('concorrer', 0.8861750960350037),\n",
       " ('indique', 0.8834351897239685),\n",
       " ('impeachment', 0.8762634992599487),\n",
       " ('indeferir', 0.876238226890564)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2vec.most_similar(\"lula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['a', 'nova', 'pesquisa', 'datafolha', 'divulgada', 'na', 'noite', 'desta', 'sexta', 'feira', 'confirma', 'as', 'tendências', 'apontadas', 'por', 'outras', 'pesquisas', 'fernando', 'haddad', 'pt', 'segue', 'em', 'sua', 'ascensão', 'desta', 'vez', 'com', 'pontos', 'além', 'do', 'último', 'levantamento']\n",
    "s = ' '.join(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
