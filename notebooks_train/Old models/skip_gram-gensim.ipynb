{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carolinesilva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carolinesilva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Text Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_filenames = ['datasets/el_pais/all_news.txt', 'datasets/el_pais/all_news2.txt', \n",
    "                  'datasets/el_pais/all_news3.txt', 'datasets/el_pais/all_news4.txt',\n",
    "                 'datasets/folha/all_news.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group all text into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'datasets/el_pais/all_news.txt'...\n",
      "Corpus is now 2217835 characters long\n",
      "Reading 'datasets/el_pais/all_news2.txt'...\n",
      "Corpus is now 4407384 characters long\n",
      "Reading 'datasets/el_pais/all_news3.txt'...\n",
      "Corpus is now 5466038 characters long\n",
      "Reading 'datasets/el_pais/all_news4.txt'...\n",
      "Corpus is now 6173125 characters long\n",
      "Reading 'datasets/folha/all_news.txt'...\n",
      "Corpus is now 6554289 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Lower case letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49801 Frases\n",
      "a nova pesquisa datafolha, divulgada na noite desta sexta-feira, confirma as tendências apontadas por outras pesquisas: fernando haddad (pt) segue em sua ascensão, desta vez com 6 pontos além do último levantamento.\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_sentences),'Frases')\n",
    "print(raw_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Remove ponctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Zçáíúéóàêôãõ]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 1,065,301 words\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(s) for s in sentences])\n",
    "print(\"The book corpus contains {0:,} words\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a nova pesquisa datafolha, divulgada na noite desta sexta-feira, confirma as tendências apontadas por outras pesquisas: fernando haddad (pt) segue em sua ascensão, desta vez com 6 pontos além do último levantamento.\n",
      "['a', 'nova', 'pesquisa', 'datafolha', 'divulgada', 'na', 'noite', 'desta', 'sexta', 'feira', 'confirma', 'as', 'tendências', 'apontadas', 'por', 'outras', 'pesquisas', 'fernando', 'haddad', 'pt', 'segue', 'em', 'sua', 'ascensão', 'desta', 'vez', 'com', 'pontos', 'além', 'do', 'último', 'levantamento']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[0])\n",
    "print(sentence_to_wordlist(raw_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"datasets/stopwords.txt\", \"r\")\n",
    "lines = text_file.readlines()\n",
    "lines = list(map(lambda x: x.replace('\\n','').replace(' ',''), lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    new_sentences.append(list(filter(lambda x: x not in lines , s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nova', 'pesquisa', 'datafolha', 'divulgada', 'noite', 'desta', 'sexta', 'feira', 'confirma', 'tendências', 'apontadas', 'outras', 'pesquisas', 'fernando', 'haddad', 'pt', 'segue', 'ascensão', 'desta', 'vez', 'pontos', 'além', 'último', 'levantamento']\n"
     ]
    }
   ],
   "source": [
    "print(new_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 596,840 tokens/words\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in new_sentences])\n",
    "print(\"The book corpus contains {0:,} words\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Skip Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "num_features  \n",
    "min_word_count  \n",
    "num_workers  \n",
    "context_size  \n",
    "downsampling  \n",
    "seed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Dimensionality of Word Vectors\n",
    "# More dimensions, more computationally expensive to train but also more accurate\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "# More workers == More faster\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "print(num_workers)\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "# random number generator\n",
    "# deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sg=1\n",
    "skip-gram is used; \n",
    "\n",
    "sg=0\n",
    "CBOW is used;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary from a sequence of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 14:29:17,585 : INFO : collecting all words and their counts\n",
      "2019-01-12 14:29:17,587 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-12 14:29:17,663 : INFO : PROGRESS: at sentence #10000, processed 116703 words, keeping 16607 word types\n",
      "2019-01-12 14:29:17,752 : INFO : PROGRESS: at sentence #20000, processed 235511 words, keeping 24550 word types\n",
      "2019-01-12 14:29:17,828 : INFO : PROGRESS: at sentence #30000, processed 351021 words, keeping 29974 word types\n",
      "2019-01-12 14:29:17,923 : INFO : PROGRESS: at sentence #40000, processed 469446 words, keeping 34182 word types\n",
      "2019-01-12 14:29:18,030 : INFO : collected 38727 word types from a corpus of 596840 raw words and 49801 sentences\n",
      "2019-01-12 14:29:18,031 : INFO : Loading a fresh vocabulary\n",
      "2019-01-12 14:29:18,437 : INFO : effective_min_count=3 retains 17786 unique words (45% of original 38727, drops 20941)\n",
      "2019-01-12 14:29:18,438 : INFO : effective_min_count=3 leaves 570101 word corpus (95% of original 596840, drops 26739)\n",
      "2019-01-12 14:29:18,542 : INFO : deleting the raw counts dictionary of 38727 items\n",
      "2019-01-12 14:29:18,544 : INFO : sample=0.001 downsamples 15 most-common words\n",
      "2019-01-12 14:29:18,546 : INFO : downsampling leaves estimated 565230 word corpus (99.1% of prior 570101)\n",
      "2019-01-12 14:29:18,652 : INFO : estimated required memory for 17786 words and 300 dimensions: 51579400 bytes\n",
      "2019-01-12 14:29:18,654 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model2vec.build_vocab(new_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 17786\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(model2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49801"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2vec.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 14:42:17,813 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-01-12 14:42:17,815 : INFO : training model with 4 workers on 17786 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2019-01-12 14:42:18,984 : INFO : EPOCH 1 - PROGRESS: at 22.24% examples, 107020 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:19,990 : INFO : EPOCH 1 - PROGRESS: at 49.60% examples, 127282 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:21,064 : INFO : EPOCH 1 - PROGRESS: at 76.60% examples, 131937 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:21,941 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 14:42:21,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 14:42:21,999 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 14:42:22,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 14:42:22,044 : INFO : EPOCH - 1 : training on 596840 raw words (565305 effective words) took 4.2s, 134144 effective words/s\n",
      "2019-01-12 14:42:23,170 : INFO : EPOCH 2 - PROGRESS: at 29.05% examples, 146132 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-12 14:42:24,370 : INFO : EPOCH 2 - PROGRESS: at 56.84% examples, 135675 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:25,486 : INFO : EPOCH 2 - PROGRESS: at 83.55% examples, 135740 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:26,082 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 14:42:26,153 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 14:42:26,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 14:42:26,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 14:42:26,192 : INFO : EPOCH - 2 : training on 596840 raw words (565267 effective words) took 4.1s, 136984 effective words/s\n",
      "2019-01-12 14:42:27,303 : INFO : EPOCH 3 - PROGRESS: at 29.05% examples, 147660 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:28,324 : INFO : EPOCH 3 - PROGRESS: at 59.95% examples, 156908 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:29,483 : INFO : EPOCH 3 - PROGRESS: at 90.31% examples, 153399 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:29,781 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 14:42:29,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 14:42:29,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 14:42:29,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 14:42:29,832 : INFO : EPOCH - 3 : training on 596840 raw words (565153 effective words) took 3.6s, 156067 effective words/s\n",
      "2019-01-12 14:42:30,864 : INFO : EPOCH 4 - PROGRESS: at 24.26% examples, 132239 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:31,930 : INFO : EPOCH 4 - PROGRESS: at 49.63% examples, 132613 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-12 14:42:32,939 : INFO : EPOCH 4 - PROGRESS: at 77.05% examples, 138432 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-12 14:42:33,763 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 14:42:33,850 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 14:42:33,859 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 14:42:33,866 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 14:42:33,867 : INFO : EPOCH - 4 : training on 596840 raw words (565237 effective words) took 4.0s, 140972 effective words/s\n",
      "2019-01-12 14:42:35,273 : INFO : EPOCH 5 - PROGRESS: at 22.24% examples, 88639 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-12 14:42:36,301 : INFO : EPOCH 5 - PROGRESS: at 44.41% examples, 101727 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:37,301 : INFO : EPOCH 5 - PROGRESS: at 66.57% examples, 108018 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-12 14:42:38,361 : INFO : EPOCH 5 - PROGRESS: at 90.31% examples, 112015 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-12 14:42:38,869 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-12 14:42:38,885 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-12 14:42:38,897 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-12 14:42:38,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-12 14:42:38,912 : INFO : EPOCH - 5 : training on 596840 raw words (565295 effective words) took 5.0s, 112311 effective words/s\n",
      "2019-01-12 14:42:38,913 : INFO : training on a 2984200 raw words (2826257 effective words) took 21.1s, 133969 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2826257, 2984200)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2vec.train(new_sentences,epochs=5, total_examples=model2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 14:42:47,279 : INFO : saving Word2Vec object under trained/model2vec.w2v, separately None\n",
      "2019-01-12 14:42:47,282 : INFO : not storing attribute vectors_norm\n",
      "2019-01-12 14:42:47,287 : INFO : not storing attribute cum_table\n",
      "2019-01-12 14:42:48,121 : INFO : saved trained/model2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "model2vec.save(os.path.join(\"trained\", \"model2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-12 14:42:50,554 : INFO : loading Word2Vec object from trained/model2vec.w2v\n",
      "2019-01-12 14:42:50,912 : INFO : loading wv recursively from trained/model2vec.w2v.wv.* with mmap=None\n",
      "2019-01-12 14:42:50,912 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-01-12 14:42:50,913 : INFO : loading vocabulary recursively from trained/model2vec.w2v.vocabulary.* with mmap=None\n",
      "2019-01-12 14:42:50,915 : INFO : loading trainables recursively from trained/model2vec.w2v.trainables.* with mmap=None\n",
      "2019-01-12 14:42:50,919 : INFO : setting ignored attribute cum_table to None\n",
      "2019-01-12 14:42:50,920 : INFO : loaded trained/model2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "model2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"model2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality\n",
    "\n",
    "t-distributed Stochastic Neighbor Embedding.\n",
    "\n",
    "t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
    "\n",
    "It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Raw Vectors to Represent - wv.syn0\n",
    "\n",
    "The raw vectors array of words in a Word2Vec or Doc2Vec model is available in model.wv.syn0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_word_vectors_matrix = model2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49801"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17786"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17786"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Graph and Relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17786"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "vocab = list(model2vec.wv.vocab)\n",
    "X = model2vec[vocab]\n",
    "df = pd.DataFrame(all_word_vectors_matrix_2d, index=vocab, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nova</th>\n",
       "      <td>55.469185</td>\n",
       "      <td>28.590986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pesquisa</th>\n",
       "      <td>47.235996</td>\n",
       "      <td>-12.171621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datafolha</th>\n",
       "      <td>20.867083</td>\n",
       "      <td>48.584522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divulgada</th>\n",
       "      <td>-55.276630</td>\n",
       "      <td>11.864432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noite</th>\n",
       "      <td>41.461628</td>\n",
       "      <td>14.854955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desta</th>\n",
       "      <td>-10.946574</td>\n",
       "      <td>-57.402309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexta</th>\n",
       "      <td>50.063782</td>\n",
       "      <td>-21.332224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feira</th>\n",
       "      <td>9.637318</td>\n",
       "      <td>-51.344612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confirma</th>\n",
       "      <td>11.616053</td>\n",
       "      <td>-28.768379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tendências</th>\n",
       "      <td>54.119797</td>\n",
       "      <td>-14.424649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    x          y\n",
       "nova        55.469185  28.590986\n",
       "pesquisa    47.235996 -12.171621\n",
       "datafolha   20.867083  48.584522\n",
       "divulgada  -55.276630  11.864432\n",
       "noite       41.461628  14.854955\n",
       "desta      -10.946574 -57.402309\n",
       "sexta       50.063782 -21.332224\n",
       "feira        9.637318 -51.344612\n",
       "confirma    11.616053 -28.768379\n",
       "tendências  54.119797 -14.424649"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_region(x_bounds, y_bounds):\n",
    "#     slice = points\n",
    "#     ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "#     for i, point in slice.iterrows():\n",
    "#         ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_region(x_bounds=(1, 2), y_bounds=(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-01-12 15:17:31,489 : INFO : precomputing L2-norms of word weight vectors\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('petista', 0.7718538045883179),\n",
       " ('inácio', 0.6719338893890381),\n",
       " ('virtualmente', 0.5995986461639404),\n",
       " ('petistas', 0.597693145275116),\n",
       " ('inscrição', 0.5877524614334106),\n",
       " ('aceitação', 0.5820702910423279),\n",
       " ('impediria', 0.5805680751800537),\n",
       " ('contradições', 0.5790199041366577),\n",
       " ('substituto', 0.5783867239952087),\n",
       " ('judice', 0.5773203372955322)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2vec.most_similar(\"lula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Palavra x esta para y, assim como a esta para b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = model2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"doria\", \"paulo\", \"haddad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
